{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e604553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train-v2.0.json\", 'r') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ac4b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available questions: 357\n"
     ]
    }
   ],
   "source": [
    "# get the available questions and answers for a given topic\n",
    "def get_qa(topic, data):\n",
    "    q = []\n",
    "    a = []\n",
    "    for d in data['data']:\n",
    "        if d['title']==topic:\n",
    "            for paragraph in d['paragraphs']:\n",
    "                for qa in paragraph['qas']:\n",
    "                    if not qa['is_impossible']:\n",
    "                        q.append(qa['question'])\n",
    "                        a.append(qa['answers'][0]['text'])\n",
    "            return q,a\n",
    "\n",
    "questions, answers = get_qa(topic='Premier_League', data=data)\n",
    "\n",
    "print(\"Number of available questions: {}\".format(len(questions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a45b555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/dev/.local/lib/python3.10/site-packages (4.27.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dev/.local/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/dev/.local/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dev/.local/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/dev/.local/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dev/.local/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in /home/dev/.local/lib/python3.10/site-packages (from transformers) (3.10.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dev/.local/lib/python3.10/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dev/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a807572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved to /home/dev/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f509cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers[torch] in /home/dev/.local/lib/python3.10/site-packages (4.27.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dev/.local/lib/python3.10/site-packages (from transformers[torch]) (2023.3.23)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers[torch]) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dev/.local/lib/python3.10/site-packages (from transformers[torch]) (23.0)\n",
      "Requirement already satisfied: filelock in /home/dev/.local/lib/python3.10/site-packages (from transformers[torch]) (3.10.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dev/.local/lib/python3.10/site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dev/.local/lib/python3.10/site-packages (from transformers[torch]) (1.24.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/dev/.local/lib/python3.10/site-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/dev/.local/lib/python3.10/site-packages (from transformers[torch]) (0.13.2)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers[torch]) (2.25.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.7 in /home/dev/.local/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dev/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (10.9.0.58)\n",
      "Requirement already satisfied: jinja2 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: sympy in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (11.7.101)\n",
      "Requirement already satisfied: networkx in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (2.14.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/dev/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (10.2.10.91)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.7->transformers[torch]) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.7->transformers[torch]) (59.6.0)\n",
      "Requirement already satisfied: lit in /home/dev/.local/lib/python3.10/site-packages (from triton==2.0.0->torch!=1.12.0,>=1.7->transformers[torch]) (16.0.0)\n",
      "Requirement already satisfied: cmake in /home/dev/.local/lib/python3.10/site-packages (from triton==2.0.0->torch!=1.12.0,>=1.7->transformers[torch]) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dev/.local/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.7->transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/dev/.local/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.7->transformers[torch]) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/dev/.local/lib/python3.10/site-packages (2.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dev/.local/lib/python3.10/site-packages (from datasets) (1.24.2)\n",
      "Requirement already satisfied: aiohttp in /home/dev/.local/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/dev/.local/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/dev/.local/lib/python3.10/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: packaging in /home/dev/.local/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/dev/.local/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: responses<0.19 in /home/dev/.local/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in /home/dev/.local/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/dev/.local/lib/python3.10/site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/dev/.local/lib/python3.10/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/dev/.local/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/dev/.local/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dev/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dev/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/dev/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/dev/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/dev/.local/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dev/.local/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/dev/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dev/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: filelock in /home/dev/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.4)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /usr/lib/python3/dist-packages (from responses<0.19->datasets) (1.26.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/dev/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\r\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (3.3)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers[torch]\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b227eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "\n",
    "def get_model(model_name):\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "  \n",
    "model, tokenizer = get_model(model_name=\"paraphrase-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "877f6b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([3, 384])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "# source: https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    \n",
    "    input_mask_expanded = (\n",
    "      attention_mask\n",
    "      .unsqueeze(-1)\n",
    "      .expand(token_embeddings.size())\n",
    "      .float()\n",
    "    )\n",
    "    \n",
    "    pool_emb = (\n",
    "      torch.sum(token_embeddings * input_mask_expanded, 1) \n",
    "      / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    )\n",
    "    \n",
    "    return pool_emb\n",
    "\n",
    "def get_embeddings(questions, tokenizer, model):\n",
    "  # Tokenize sentences\n",
    "  encoded_input = tokenizer(questions, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "  # Compute token embeddings\n",
    "  with torch.no_grad():\n",
    "      model_output = model(**encoded_input)\n",
    "\n",
    "  # Average pooling\n",
    "  embeddings = mean_pooling(model_output, encoded_input['attention_mask']) \n",
    "  \n",
    "  return embeddings\n",
    "\n",
    "embeddings = get_embeddings(questions[:3], tokenizer, model)\n",
    "print(\"Embeddings shape: {}\".format(embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da8372d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([71.4030, 59.8726, 23.9430])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_question = 'Which days have the most events played at?'\n",
    "new_embedding = get_embeddings([new_question], tokenizer, model)\n",
    "\n",
    "# squared Euclidean distance between sample questions and new_question\n",
    "((embeddings - new_embedding)**2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2f666e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAEmbedder:\n",
    "  def __init__(self, model_name=\"paraphrase-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Defines a QA embedding model. This is, given a set of questions,\n",
    "    this class returns the corresponding embedding vectors.\n",
    "    \n",
    "    Args:\n",
    "      model_name (`str`): Directory containing the necessary tokenizer\n",
    "        and model files.\n",
    "    \"\"\"\n",
    "    self.model = None\n",
    "    self.tokenizer = None\n",
    "    self.model_name = model_name\n",
    "    self.set_model(model_name)\n",
    "  \n",
    "  \n",
    "  def get_model(self, model_name):\n",
    "    \"\"\"\n",
    "    Loads a general tokenizer and model using pytorch\n",
    "    'AutoTokenizer' and 'AutoModel'\n",
    "    \n",
    "    Args:\n",
    "      model_name (`str`): Directory containing the necessary tokenizer\n",
    "        and model files.\n",
    "    \"\"\"\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "  \n",
    "  \n",
    "  def set_model(self, model_name):\n",
    "    \"\"\"\n",
    "    Sets a general tokenizer and model using the 'self.get_model'\n",
    "    method.\n",
    "    \n",
    "    Args:\n",
    "      model_name (`str`): Directory containing the necessary tokenizer\n",
    "        and model files.\n",
    "    \"\"\"\n",
    "    self.model, self.tokenizer = self.get_model(self.model_name)\n",
    "  \n",
    "  \n",
    "  def _mean_pooling(self, model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Internal method that takes a model output and an attention\n",
    "    mask and outputs a mean pooling layer.\n",
    "    \n",
    "    Args:\n",
    "      model_output (`torch.Tensor`): output from the QA model\n",
    "      attention_mask (`torch.Tensor`): attention mask defined in the QA tokenizer\n",
    "      \n",
    "    Returns:\n",
    "      The averaged tensor.\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output[0]\n",
    "    \n",
    "    input_mask_expanded = (\n",
    "      attention_mask\n",
    "      .unsqueeze(-1)\n",
    "      .expand(token_embeddings.size())\n",
    "      .float()\n",
    "    )\n",
    "    \n",
    "    pool_emb = (\n",
    "      torch.sum(token_embeddings * input_mask_expanded, 1) \n",
    "      / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    )\n",
    "    \n",
    "    return pool_emb\n",
    "  \n",
    "  \n",
    "  def get_embeddings(self, questions, batch=32):\n",
    "    \"\"\"\n",
    "    Gets the corresponding embeddings for a set of input 'questions'.\n",
    "    \n",
    "    Args:\n",
    "      questions (`list` of `str`): List of strings defining the questions to be embedded\n",
    "      batch (`int`): Performs the embedding job 'batch' questions at a time\n",
    "      \n",
    "    Returns:\n",
    "      The embedding vectors.\n",
    "    \"\"\"\n",
    "    question_embeddings = []\n",
    "    for i in range(0, len(questions), batch):\n",
    "    \n",
    "        # Tokenize sentences\n",
    "        encoded_input = self.tokenizer(questions[i:i+batch], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "\n",
    "        # Perform mean pooling\n",
    "        batch_embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        question_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    question_embeddings = torch.cat(question_embeddings, dim=0)\n",
    "    return question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465e1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QASearcher:\n",
    "  def __init__(self, model_name=\"paraphrase-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Defines a QA Search model. This is, given a new question it searches\n",
    "    the most similar questions in a set 'context' and returns both the best\n",
    "    question and associated answer.\n",
    "    \n",
    "    Args:\n",
    "      model_name (`str`): Directory containing the necessary tokenizer\n",
    "        and model files.\n",
    "    \"\"\"\n",
    "    self.answers = None\n",
    "    self.questions = None\n",
    "    self.question_embeddings = None\n",
    "    self.embedder = QAEmbedder(model_name=model_name)\n",
    "  \n",
    "  \n",
    "  def set_context_qa(self, questions, answers):\n",
    "    \"\"\"\n",
    "    Sets the QA context to be used during search.\n",
    "    \n",
    "    Args:\n",
    "      questions (`list` of `str`):  List of strings defining the questions to be embedded\n",
    "      answers (`list` of `str`): Best answer for each question in 'questions'\n",
    "    \"\"\"\n",
    "    self.answers = answers\n",
    "    self.questions = questions\n",
    "    self.question_embeddings = self.get_q_embeddings(questions)\n",
    "  \n",
    "  \n",
    "  def get_q_embeddings(self, questions):\n",
    "    \"\"\"\n",
    "    Gets the embeddings for the questions in 'context'.\n",
    "    \n",
    "    Args:\n",
    "      questions (`list` of `str`):  List of strings defining the questions to be embedded\n",
    "    \n",
    "    Returns:\n",
    "      The embedding vectors.\n",
    "    \"\"\"\n",
    "    question_embeddings = self.embedder.get_embeddings(questions)\n",
    "    question_embeddings  = torch.nn.functional.normalize(question_embeddings, p=2, dim=1)\n",
    "    return question_embeddings.transpose(0,1)\n",
    "  \n",
    "  \n",
    "  def cosine_similarity(self, questions, batch=32):\n",
    "    \"\"\"\n",
    "    Gets the cosine similarity between the new questions and the 'context' questions.\n",
    "    \n",
    "    Args:\n",
    "      questions (`list` of `str`):  List of strings defining the questions to be embedded\n",
    "      batch (`int`): Performs the embedding job 'batch' questions at a time\n",
    "    \n",
    "    Returns:\n",
    "      The cosine similarity\n",
    "    \"\"\"\n",
    "    question_embeddings = self.embedder.get_embeddings(questions, batch=batch)\n",
    "    question_embeddings = torch.nn.functional.normalize(question_embeddings, p=2, dim=1)\n",
    "    \n",
    "    cosine_sim = torch.mm(question_embeddings, self.question_embeddings)\n",
    "    \n",
    "    return cosine_sim\n",
    "  \n",
    "  \n",
    "  def get_answers(self, questions, batch=32):\n",
    "    \"\"\"\n",
    "    Gets the best answers in the stored 'context' for the given new 'questions'.\n",
    "    \n",
    "    Args:\n",
    "      questions (`list` of `str`):  List of strings defining the questions to be embedded\n",
    "      batch (`int`): Performs the embedding job 'batch' questions at a time\n",
    "    \n",
    "    Returns:\n",
    "      A `list` of `dict`'s containing the original question ('orig_q'), the most similar\n",
    "      question in the context ('best_q') and the associated answer ('best_a').\n",
    "    \"\"\"\n",
    "    similarity = self.cosine_similarity(questions, batch=batch)\n",
    "    \n",
    "    response = []\n",
    "    for i in range(similarity.shape[0]):\n",
    "      best_ix = similarity[i].argmax()\n",
    "      best_q = self.questions[best_ix]\n",
    "      best_a = self.answers[best_ix]\n",
    "      \n",
    "      response.append(\n",
    "        {\n",
    "          'orig_q':questions[i],\n",
    "          'best_q':best_q,\n",
    "          'best_a':best_a,\n",
    "        }\n",
    "      )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "091a82d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: uvicorn in /home/dev/.local/lib/python3.10/site-packages (0.21.1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/dev/.local/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/lib/python3/dist-packages (from uvicorn) (8.0.3)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fastapi in /home/dev/.local/lib/python3.10/site-packages (0.95.0)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /home/dev/.local/lib/python3.10/site-packages (from fastapi) (1.10.7)\n",
      "Requirement already satisfied: starlette<0.27.0,>=0.26.1 in /home/dev/.local/lib/python3.10/site-packages (from fastapi) (0.26.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/dev/.local/lib/python3.10/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi) (4.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/dev/.local/lib/python3.10/site-packages (from starlette<0.27.0,>=0.26.1->fastapi) (3.6.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/dev/.local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install uvicorn\n",
    "! pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ec4e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "qa_search = QASearcher()\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/set_context\")\n",
    "async def set_context(data:Request):\n",
    "  \"\"\"\n",
    "  Fastapi POST method that sets the QA context for search.\n",
    "  \n",
    "  Args:\n",
    "    data(`dict`): Two fields required 'questions' (`list` of `str`)\n",
    "      and 'answers' (`list` of `str`)\n",
    "  \"\"\"\n",
    "  data = await data.json()\n",
    "  \n",
    "  qa_search.set_context_qa(\n",
    "    data['questions'], \n",
    "    data['answers']\n",
    "  )\n",
    "  return {\"message\": \"Search context set\"}\n",
    "\n",
    "\n",
    "@app.post(\"/get_answer\")\n",
    "async def get_answer(data:Request):\n",
    "  \"\"\"\n",
    "  Fastapi POST method that gets the best question and answer \n",
    "  in the set context.\n",
    "  \n",
    "  Args:\n",
    "    data(`dict`): One field required 'questions' (`list` of `str`)\n",
    "  \n",
    "  Returns:\n",
    "    A `dict` containing the original question ('orig_q'), the most similar\n",
    "    question in the context ('best_q') and the associated answer ('best_a').\n",
    "  \"\"\"\n",
    "  data = await data.json()\n",
    "  \n",
    "  response = qa_search.get_answers(data['questions'], batch=1)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21ad9975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Search context set'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "json_data = {\n",
    "  'questions':questions,\n",
    "  'answers':answers,\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "  'http://0.0.0.0:8000/set_context',\n",
    "  json=json_data\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce3b0dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_q : How many teams compete in the Premier League ?\n",
      "best_q : How many clubs are currently in the Premier League?\n",
      "best_a : 20\n",
      "\n",
      "orig_q : When does the Premier League starts and finishes ?\n",
      "best_q : When does the Premier League have its playing season?\n",
      "best_a : During the course of a season (from August to May)\n",
      "\n",
      "orig_q : Who has the highest number of goals in the Premier League ?\n",
      "best_q : Who has the record for most goals in the Premier League?\n",
      "best_a : Newcastle United striker Alan Shearer holds the record for most Premier League goals with 260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_questions = [\n",
    "    'How many teams compete in the Premier League ?',\n",
    "    'When does the Premier League starts and finishes ?',\n",
    "    'Who has the highest number of goals in the Premier League ?',\n",
    "]\n",
    "\n",
    "json_data = {\n",
    "  'questions':new_questions,\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "  'http://0.0.0.0:8000/get_answer',\n",
    "  json=json_data\n",
    ")\n",
    "\n",
    "for d in response.json():\n",
    "  print('\\n'.join([\"{} : {}\".format(k, v) for k,v in d.items()])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba67964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
